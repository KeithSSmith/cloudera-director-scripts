#
# Copyright (c) 2016 Cloudera, Inc. All rights reserved.
#

#
# Sample Cloudera Director configuration file based on the Cloudera Azure reference architecture:
# http://www.cloudera.com/documentation/other/reference-architecture/PDF/cloudera_ref_arch_azure.pdf
#
# This will create a Highly Available cluster with 3 master nodes and N worker nodes,
# SSSD to AD integration, with Kerberos enabled, external authentication and authorization across the cluster.
#


#
# Cluster name
# If environmentName and deploymentName are not defined they will get the value of 'name'.
# Must be unique.
#

name: ${CLUSTER_NAME}

#
# Environment name
#

environmentName: ${ENVIRONMENT_NAME}

#
# Deployment name
# Used to name the Cloudera Manager instance in Cloudera Director.
# Must be unique.
#

deploymentName: ${CLOUDERA_MANAGER_NAME}


#
# Cloud provider configuration (credentials, region, and management/authentication endpoints)
#

provider {
    type: azure

    #
    # ID of Azure region to use. NOTE: region must support Premium Storage
    # See: https://azure.microsoft.com/en-us/regions/#services
    #

    region: ${AZURE_REGION}

    #
    # Azure Resource Management URL.
    #

    mgmtUrl: "https://management.core.windows.net/"

    #
    # Azure Active Directory Subscription ID.
    #

    subscriptionId: ${AZURE_SUBSCRIPTION_ID}

    #
    # Azure Active Directory URL.
    #

    aadUrl: "https://login.windows.net/"

    #
    # Tenant ID (from AAD)
    #

    tenantId: ${AZURE_TENANT_ID}

    #
    # Azure Active Directory Application Client ID.
    #

    clientId: ${AZURE_CLIENT_ID}

    #
    # Client Secret
    #

    clientSecret: ${AZURE_CLIENT_SECRET}
}


#
# SSH credentials to use to connect to the machines
#

ssh {
    username: ${SSH_USERNAME}
    privateKey: ${SSH_PEM_PATH} # with an absolute path to .pem file
}


#
# Common variable definitions
#
# These are the key / value pairs used to define instance templates. They are defined here and
# referenced throughout this config file.
# More about HOCON substitution: https://github.com/typesafehub/config/blob/master/HOCON.md#substitutions
#
# The format of this section:
#   - base: represents the core config fields that are common to all nodes
#   - master-base: represents the core config fields that are common to all master nodes
#   - master-1: represents the core config fields that are common to master-1 nodes
#   - master-2: represents the core config fields that are common to master-2 nodes
#   - worker: represents the core config fields that are common to worker nodes
#   - edge: represents the core config fields that are common to edge nodes
#
#
# Instance Template Breakdown
# An instance template configuration consists of the following fields. Unless otherwise specified,
# all fields are required.
#
# - image: The image ID used for instances is an alias defined in the plugin configuration file.
#
# - type: The VM type. See the Azure RA for more detail.
#
# - computeResourceGroup: Resource Group for the deployment.  The Resource Group you specify must
#   exist within the region you selected.
#   See: https://azure.microsoft.com/en-us/documentation/articles/resource-group-overview/
#
# - networkSecurityGroupResourceGroup: The Resource Group for the Network Security Group. The
#   Resource Group you specify must exist within the region you selected.
#   See: https://azure.microsoft.com/en-us/documentation/articles/resource-group-overview/
#
# - networkSecurityGroup: The Network Security Group for this instance type, this has to be
#   within the networkSecurityGroupResourceGroup. NSG configuration allows you to limit access to
#   the VM with firewall-like rules.
#   See: https://azure.microsoft.com/en-us/documentation/articles/virtual-networks-nsg/
#
# - virtualNetworkResourceGroup: The Resource Group for the Virtual Network. The Resource Group you
#   specify must exist within the region you selected and should be the same for all instances that
#   will be used in the same cluster.
#   See: https://azure.microsoft.com/en-us/documentation/articles/resource-group-overview/
#
# - virtualNetwork: The Azure Virtual Network that will be used, this has to be within the
#   virtualNetworkResourceGroup and should be the same for all instances that will be used in the
#   same cluster.
#   See: https://azure.microsoft.com/en-us/documentation/services/virtual-network/
#
# - subnetName: The name of the Subnet that will be used, this has to be within the virtualNetwork.
#
# - instanceNamePrefix: Prefix for VM name and hostname of the VM. The VM name will have the
#   folloing format:
#       instanceNamePrefix-{UUID}
#   where {UUID} is generated by the Cloudera Director server.
#
# - hostFqdnSuffix: Hostname FQDN Suffix. This is the DNS domain you configured in your custom DNS
#   server. Example values are: cdh-cluster.internal, cluster.your-company-name.com. The host FQDN
#   is configured on the VMs with the following format:
#       {instanceNamePrefix}-{truncated-UUID}.hostFqdnSuffix
#
# - availabilitySet: Availability Set for this instance type.  Machines within the same availability
#   set will have staggared maintanance times. With a default availability set configuration no more
#   than 1/5 machines will be offline at a time (rounded up).
#   See: https://azure.microsoft.com/en-us/documentation/articles/virtual-machines-linux-manage-availability/
#   Sharing Availability Set between master and worker nodes is strongly not recommended.
#
# - publicIP: Should this instance type have Azure Public IP Address and DNS Label?  If Yes, the
#   machine will have a publically resolvable hostname with the folling format:
#       {instanceNamePrefix}-{UUID}.{region}.coudapp.azure.com
#   Allowed values: Yes, No
#
# - storageAccountType: The storage account type to use. The dataDiskSize parameter should be
#   updated based on the storage account type used.
#   The current allowed values are:
#       - PremiumLRS
#       - StandardLRS
#   See the RA for the supported ways to use standard storage:
#   http://www.cloudera.com/documentation/other/reference-architecture/PDF/cloudera_ref_arch_azure.pdf
#
# - dataDiskCount: The number of data drives. The size can be specified with `dataDiskSize`
#   Data drives are mounted on /data0 .. /data[n]
#       /data0 - Dedicated Log Device
#   For Masters-1:
#       /data1 - HDFS JournalNode Data
#       /data2 - Zookeeper Data / DataLog
#       /data3 - NameNode Data
#   For Masters-2:
#       /data1 - HDFS JournalNode Data
#       /data2 - Zookeeper Data / DataLog
#   For Workers:
#       /data1 .. /data[n] will be used for HDFS data
#
# - dataDiskSize: The size of each data drive.
#   For disks allocated in a premium storage account, only the following GB values are allowed:
#       512 (P20 disk)
#       1023 (P30 disk) (1023, not 1024)
#   For disks alllocated in a standard storage account, any size between 1 and 1023 inclusive can
#   be used.
#   See https://azure.microsoft.com/en-us/documentation/articles/storage-introduction/ and
#   https://azure.microsoft.com/en-us/documentation/articles/storage-premium-storage/
#
# - tags (optional): Additional tags to help label resources within Azure
#
# - bootstrapScript: The bootstrap script (see the bootstrap-script section)
#

common-instanceTemplate {

    # Core config fields that are common to all node types
    base {
        type: ${AZURE_NODE_SIZE}
        image: ${AZURE_OS}
        networkSecurityGroupResourceGroup: ${AZURE_NETWORK_SG_RG}
        networkSecurityGroup: ${AZURE_NETWORK_SG}
        virtualNetworkResourceGroup: ${AZURE_VNET_RG}
        virtualNetwork: ${AZURE_VNET}
        subnetName: ${AZURE_SUBNET}
        hostFqdnSuffix: ${DOMAIN}
        tags {
            owner: ${TAGS_OWNER}
            project: ${TAGS_PROJECT}
        }
    }

    # Core config fields that are common to all master nodes
    master-base {
        computeResourceGroup: ${AZURE_COMPUTE_RG}
        availabilitySet: ${AZURE_AVAILABILITY_SET}
        instanceNamePrefix: ${AZURE_MASTER_NAME_PREFIX}
        storageAccountType: ${AZURE_MASTER_STORAGE_TYPE}
        dataDiskSize: ${AZURE_MASTER_DISK_SIZE}
        publicIP: ${AZURE_PUBLIC_IP}
    }

    # Config fields for edge nodes
    management {
        computeResourceGroup: ${AZURE_COMPUTE_RG}
        availabilitySet: ${AZURE_AVAILABILITY_SET}
        instanceNamePrefix: ${AZURE_MGMT_NAME_PREFIX}
        storageAccountType: ${AZURE_MGMT_STORAGE_TYPE}
        dataDiskCount: ${AZURE_MGMT_DISK_COUNT}
        dataDiskSize: ${AZURE_MGMT_DISK_SIZE}
        publicIP: ${AZURE_PUBLIC_IP}
    }

    # Config fields for master-1 nodes
    master-1 {
        dataDiskCount: ${AZURE_MASTER1_DISK_COUNT}
    }

    # Config fields for master-2 nodes
    master-2 {
        dataDiskCount: ${AZURE_MASTER2_DISK_COUNT}
    }

    # Config fields for worker nodes
    worker {
        computeResourceGroup: ${AZURE_COMPUTE_RG}
        availabilitySet: ${AZURE_AVAILABILITY_SET}
        instanceNamePrefix: ${AZURE_WORKER_NAME_PREFIX}
        storageAccountType: ${AZURE_WORKER_STORAGE_TYPE}
        dataDiskCount: ${AZURE_WORKER_DISK_COUNT}
        dataDiskSize: ${AZURE_WORKER_DISK_SIZE}
        publicIP: ${AZURE_PUBLIC_IP}
    }

    # Config fields for edge nodes
    edge {
        computeResourceGroup: ${AZURE_COMPUTE_RG}
        availabilitySet: ${AZURE_AVAILABILITY_SET}
        instanceNamePrefix: ${AZURE_EDGE_NAME_PREFIX}
        storageAccountType: ${AZURE_EDGE_STORAGE_TYPE}
        dataDiskCount: ${AZURE_EDGE_DISK_COUNT}
        dataDiskSize: ${AZURE_EDGE_DISK_SIZE}
        publicIP: ${AZURE_PUBLIC_IP}
    }
}


#
# Bootstrap script
#
# The os-generic bootstrap script will be run after the VM boots up for the first time. This must
# be used to set up preconditions for successful cluster deployment. Director will restart the
# host after the bootstrap script has run.
#
# The example below is an os-generic script that supports these OSes:
#   - CentOS 6.7
#   - CentOS 7.2
#   - RHEL 6.7
#   - RHEL 7.2
#
# The script prepares the OS for cluster installation. It also configures a dhclient or
# NetworkManager hook (depending on OS) to register the A record and PTR record with the DNS server
# configured for the VNET to satisfy proper forward and reverse DNS resolution. Azure's default DNS
# currently does not support Reverse Lookup on private IP Addresses, which is a requirement for
# CDH. See the following link for an example BIND setup to satisfy this requirement:
# http://www.cloudera.com/documentation/director/latest/topics/director_get_started_azure_ddns.html
#
# The scrip also sets required settings for RHEL.
#

bootstrap-script {
    os-generic : """#!/bin/sh

#
# This script will bootstrap these OSes:
#   - CentOS 6.7
#   - CentOS 7.2
#   - RHEL 6.7
#   - RHEL 7.2
#
# Notes and notible differences between OSes:
#   - CentOS 6.7 and RHEL 6.7 use dhclient
#   - CentOS 7.2 and RHEL 7.2 use NetworkManager
#


#
# Functions
#

# writing dhclient-exit-hooks is the same for CentOS 6.7 and RHEL 6.7
# function not indented so EOF works
dhclient_67()
{
# dhclient-exit-hooks explained in dhclient-script man page: http://linux.die.net/man/8/dhclient-script
# cat a here-doc represenation of the hooks to the appropriate file
cat > /etc/dhcp/dhclient-exit-hooks <<"EOF"
#!/bin/bash
printf "\ndhclient-exit-hooks running...\n\treason:%s\n\tinterface:%s\n" "${reason:?}" "${interface:?}"
# only execute on the primary nic
if [ "$interface" != "eth0" ]
then
    exit 0;
fi
# when we have a new IP, perform nsupdate
if [ "$reason" = BOUND ] || [ "$reason" = RENEW ] ||
[ "$reason" = REBIND ] || [ "$reason" = REBOOT ]
then
    printf "\tnew_ip_address:%s\n" "${new_ip_address:?}"
    host=$(hostname -s)
    domain=$(hostname | cut -d'.' -f2- -s)
    domain=${domain:='cdh-cluster.internal'} # If no hostname (see hostFqdnSuffix_REPLACE_ME) is provided, use cdh-cluster.internal
    IFS='.' read -ra ipparts <<< "$new_ip_address"
    ptrrec="$(printf %s "$new_ip_address." | tac -s.)in-addr.arpa"
    nsupdatecmds=$(mktemp -t nsupdate.XXXXXXXXXX)
    resolvconfupdate=$(mktemp -t resolvconfupdate.XXXXXXXXXX)
    echo updating resolv.conf
    grep -iv "search" /etc/resolv.conf > "$resolvconfupdate"
    echo "search $domain" >> "$resolvconfupdate"
    cat "$resolvconfupdate" > /etc/resolv.conf
    echo "Attempting to register $host.$domain and $ptrrec"
    {
        echo "update delete $host.$domain a"
        echo "update add $host.$domain 600 a $new_ip_address"
        echo "send"
        echo "update delete $ptrrec ptr"
        echo "update add $ptrrec 600 ptr $host.$domain"
        echo "send"
    } > "$nsupdatecmds"
    nsupdate "$nsupdatecmds"
fi
#done
exit 0;
EOF
chmod 755 /etc/dhcp/dhclient-exit-hooks
service network restart
}


centos_67()
{
    echo "CentOS 6.7"

    # execute the CentOS 6.7 / RHEL 6.7 dhclient-exit-hooks setup
    dhclient_67
}


rhel_67()
{
    echo "RHEL 6.7"

    # rewrite SELINUX config to disabled and turn off enforcement
    sed -i.bak "s/^SELINUX=.*$/SELINUX=disabled/" /etc/selinux/config
    setenforce 0
    # stop firewall and disable
    service iptables stop
    chkconfig iptables off
    # update config to disable IPv6 and disable
    echo "# Disable IPv6" >> /etc/sysctl.conf
    echo "net.ipv6.conf.all.disable_ipv6 = 1" >> /etc/sysctl.conf
    echo "net.ipv6.conf.default.disable_ipv6 = 1" >> /etc/sysctl.conf
    sysctl -w net.ipv6.conf.all.disable_ipv6=1
    sysctl -w net.ipv6.conf.default.disable_ipv6=1

    # execute the CentOS 6.7 / RHEL 6.7 dhclient-exit-hooks setup
    dhclient_67
}


# writing network manager hooks is the same for CentOS 7.2 and RHEL 7.2
# function not indented so EOF works
networkmanager_72()
{
# https://github.com/cloudera/director-scripts/blob/master/azure-dns-scripts/bootstrap_dns_nm.sh
# RHEL 7.2 uses NetworkManager. Add a script to be automatically invoked when interface comes up.
cat > /etc/NetworkManager/dispatcher.d/12-register-dns <<"EOF"
#!/bin/bash
# NetworkManager Dispatch script
# Deployed by Cloudera Director Bootstrap
#
# Expected arguments:
#    $1 - interface
#    $2 - action
#
# See for info: http://linux.die.net/man/8/networkmanager
# Register A and PTR records when interface comes up
# only execute on the primary nic
if [[ "$1" != "eth0" || "$2" != "up" ]]
then
    exit 0;
fi
# when we have a new IP, perform nsupdate
new_ip_address="$DHCP4_IP_ADDRESS"
host=$(hostname -s)
domain=$(hostname | cut -d'.' -f2- -s)
domain=${domain:='cdh-cluster.internal'} # REPLACE_ME If no hostname is provided, use cdh-cluster.internal
IFS='.' read -ra ipparts <<< "$new_ip_address"
ptrrec="$(printf %s "$new_ip_address." | tac -s.)in-addr.arpa"
nsupdatecmds=$(mktemp -t nsupdate.XXXXXXXXXX)
resolvconfupdate=$(mktemp -t resolvconfupdate.XXXXXXXXXX)
echo updating resolv.conf
grep -iv "search" /etc/resolv.conf > "$resolvconfupdate"
echo "search reddog.microsoft.com" >> "$resolvconfupdate"
echo "search $domain" >> "$resolvconfupdate"
cat "$resolvconfupdate" > /etc/resolv.conf
echo "Attempting to register $host.$domain and $ptrrec"
{
    echo "update delete $host.$domain a"
    echo "update add $host.$domain 600 a $new_ip_address"
    echo "send"
    echo "update delete $ptrrec ptr"
    echo "update add $ptrrec 600 ptr $host.$domain"
    echo "send"
} > "$nsupdatecmds"
nsupdate -g "$nsupdatecmds"
exit 0;
EOF
chmod 755 /etc/NetworkManager/dispatcher.d/12-register-dns
kinit ${DNS_JOIN_USER} <<EOF
${DNS_JOIN_PASS}
EOF                  ###  This could/should be replaced by a keytab to avoid sending the password in plain text.  REPLACE_ME
systemctl restart NetworkManager
systemctl restart network
}


centos_72()
{
    echo "CentOS 7.2"

    # execute the CentOS 7.2 / RHEL 7.2 network manager setup
    networkmanager_72
}


rhel_72()
{
    echo "RHEL 7.2"

    ###  Define Variables  ###
    HOST_IP_ADDRESS=$(ip addr | grep eth0 -A2 | head -n3 | tail -n1 | awk -F'[/ ]+' '{print $3}')
    HOSTNAME=$(hostname)
    DOMAIN="REPLACE_ME"
    DOMAIN_CONTROLLER="REPLACE_ME.${DOMAIN}"
    DNS_IP="REPLACE_ME"
    AD_JOIN_USER="REPLACE_ME"
    AD_JOIN_PASS="REPLACE_ME"
    DNS_JOIN_USER="REPLACE_ME"
    DNS_JOIN_PASS="REPLACE_ME"
    REPO_IP="REPLACE_ME"
    REPO_PORT="REPLACE_ME"
    COMPUTER_OU="REPLACE_ME"            ## Example: OU=cloudera,DC=example,DC=com   This is where the Computer entity will be created inside of AD, if desired to be outside of the Computers OU.
    KEYSTORE_PASS="REPLACE_ME"
    TRUSTSTORE_PASS="REPLACE_ME"
    

    KRB5_CONF=$(mktemp -t krb5_conf.XXXXXXXXXX)
    NTP_CONF=$(mktemp -t ntp_conf.XXXXXXXXXX)
    DIRECTOR_REPO=$(mktemp -t director_repo.XXXXXXXXXX)
    SSSD_CONF=$(mktemp -t sssd_conf.XXXXXXXXXX)
    SSHD_CONF=$(mktemp -t sshd_conf.XXXXXXXXXX)

    ###  Ensure the Node and YUM are up to date.  ###
    yum clean all
    yum makecache fast
    yum -y update
    yum -y install bind-utils wget telnet redhat-lsb-core nscd rng-tools ntp

    ###  Set SELinux to permissive  ###
    sed -i.bak "s/^SELINUX=.*$/SELINUX=disabled/" /etc/selinux/config
    setenforce 0

    ###  Disable tuned so it does not overwrite sysctl.conf  ###
    service tuned stop
    systemctl disable tuned

    ###  Disable chrony so it does not conflict with ntpd installed by Director  ###
    systemctl stop chronyd
    systemctl disable chronyd

    ###  Update config to disable IPv6 and disable  ###
    echo "# Disable IPv6" >> /etc/sysctl.conf
    echo "net.ipv6.conf.all.disable_ipv6 = 1" >> /etc/sysctl.conf
    echo "net.ipv6.conf.default.disable_ipv6 = 1" >> /etc/sysctl.conf

    # swappniess is set by Director in /etc/sysctl.conf
    # Poke sysctl to have it pickup the config change.
    sysctl -p

    ### Enable and start rngd  ###
    echo 'EXTRAOPTIONS="-r /dev/urandom"' > /etc/sysconfig/rngd
    chkconfig rngd on
    service rngd start

    ###  Turn off iptables  ###
    systemctl stop firewalld
    systemctl disable firewalld

    ###  Turn on NTP with internal NTP Server  ###
    sed -e 's/server 0.rhel.pool.ntp.org iburst/#server 0.rhel.pool.ntp.org iburst/' \
     -e 's/server 1.rhel.pool.ntp.org iburst/#server 1.rhel.pool.ntp.org iburst/' \
     -e 's/server 2.rhel.pool.ntp.org iburst/#server 2.rhel.pool.ntp.org iburst/' \
     -e "s/server 3.rhel.pool.ntp.org iburst/#server 3.rhel.pool.ntp.org iburst\n# NTP Server defined manually.\nserver ${DOMAIN_CONTROLLER} prefer/" \
     /etc/ntp.conf > ${NTP_CONF}

    cat ${NTP_CONF} > /etc/ntp.conf
    systemctl restart ntpd
    ntpdate -u ${DOMAIN_CONTROLLER}

    ###  Update Timezone  ###
    rm -f /etc/localtime
    ln -s /usr/share/zoneinfo/US/Central /etc/localtime

    ###  Download and Install the MySQL Java Connector  ###
    wget "http://${REPO_IP}:${REPO_PORT}/dev.mysql.com/Connector-J/mysql-connector-java-5.1.40.tar.gz" -O /tmp/mysql-connector-java-5.1.40.tar.gz
    tar zxvf /tmp/mysql-connector-java-5.1.40.tar.gz -C /tmp/
    mkdir -p /usr/share/java/
    cp /tmp/mysql-connector-java-5.1.40/mysql-connector-java-5.1.40-bin.jar /usr/share/java/
    rm /usr/share/java/mysql-connector-java.jar
    ln -s /usr/share/java/mysql-connector-java-5.1.40-bin.jar /usr/share/java/mysql-connector-java.jar


    ###  Install Packages needed for SSSD, DNS, LDAP, and Kerberos  ###
    yum -y install realmd sssd sssd-ad samba-common adcli sssd-libwbclient openldap-devel openldap-clients pam_krb5 samba-common-tools krb5-workstation krb5-libs

    ###  Update resolv.conf  ###
    echo "search ${DOMAIN}" >> /etc/resolv.conf

    ###  Change /etc/krb5.conf ###
    echo "[logging]
 default = FILE:/var/log/krb5libs.log
 kdc = FILE:/var/log/krb5kdc.log
 admin_server = FILE:/var/log/kadmind.log

[libdefaults]
 dns_lookup_realm = false
 dns_lookup_kdc = false
 ticket_lifetime = 24h
 renew_lifetime = 7d
 forwardable = true
 default_realm = ${DOMAIN^^}
 default_ccache_name = FILE:/tmp/krb5cc_%{uid}
 udp_preference_limit = 1

[realms]
 ${DOMAIN^^} = {
  kdc = ${DOMAIN_CONTROLLER}
  admin_server = ${DOMAIN_CONTROLLER}
 }

[domain_realm]
 .${DOMAIN} = ${DOMAIN^^}
 ${DOMAIN} = ${DOMAIN^^}
" > "${KRB5_CONF}"

    cat "${KRB5_CONF}" > /etc/krb5.conf

    ###  Join the computer to the domain.  ###
    realm discover ${DOMAIN^^}
    realm join ${DOMAIN^^} -U "${AD_JOIN_USER}" --verbose --computer-ou="${COMPUTER_OU}" <<EOF
${AD_JOIN_PASS}
EOF                     ### This should be changed to a keytab; REPLACE_ME

    ###  Configure SSSD and SSH configuration.  ###
    sed -e 's/use_fully_qualified_names = True/use_fully_qualified_names = False/' \
     -e 's|fallback_homedir = /home/%u@%d|fallback_homedir = /home/%u|' \
     -e 's|services = nss, pam|services = nss, pam\n\n[nss]\noverride_homedir = /home/%u\ndefault_shell = /bin/bash|' \
     /etc/sssd/sssd.conf > ${SSSD_CONF}

    cat ${SSSD_CONF} > /etc/sssd/sssd.conf
    rm -f /var/lib/sss/db/*
    rm -f /var/lib/sss/mc/*
    systemctl restart sssd

    sed -e 's/PasswordAuthentication no/PasswordAuthentication yes/' \
     -e 's/ChallengeResponseAuthentication no/ChallengeResponseAuthentication yes/' \
     /etc/ssh/sshd_config > ${SSHD_CONF}

    cat ${SSHD_CONF} > /etc/ssh/sshd_config
    systemctl restart sshd

    # execute the CentOS 7.2 / RHEL 7.2 network manager setup
    networkmanager_72

    ###  Enable and start nscd  ###
    chkconfig nscd on
    service nscd start

    ###  Install Java 8  ###
    echo "[cloudera-director]
# Packages for Cloudera Director, Version 2, on RedHat or CentOS 7 x86_64
name=Cloudera Director
baseurl=http://${REPO_IP}:${REPO_PORT}/archive.cloudera.com/director/redhat/7/x86_64/director/2.2.0/
gpgkey = http://${REPO_IP}:${REPO_PORT}/archive.cloudera.com/director/redhat/7/x86_64/director/RPM-GPG-KEY-cloudera
gpgcheck = 1
enabled=1" > "${DIRECTOR_REPO}"

    cat "${DIRECTOR_REPO}" > /etc/yum.repos.d/cloudera-director.repo

    rpm --import "http://${REPO_IP}:${REPO_PORT}/archive.cloudera.com/director/redhat/7/x86_64/director/RPM-GPG-KEY-cloudera"

    yum clean all
    yum makecache fast
    yum -y install java --nogpgcheck

    ###  Install Java Unlimited Strength Encryption Policy Files for Java 8  ###
    wget -O /tmp/jce_policy-8.zip "http://${REPO_IP}:${REPO_PORT}/download.oracle.com/otn-pub/java/jce/8/jce_policy-8.zip"
    unzip /tmp/jce_policy-8.zip -d /tmp
    rm -f /usr/java/jdk1.8.0_60/jre/lib/security/local_policy.jar
    rm -f /usr/java/jdk1.8.0_60/jre/lib/security/US_export_policy.jar
    mv /tmp/UnlimitedJCEPolicyJDK8/local_policy.jar /usr/java/jdk1.8.0_60/jre/lib/security/local_policy.jar
    mv /tmp/UnlimitedJCEPolicyJDK8/US_export_policy.jar /usr/java/jdk1.8.0_60/jre/lib/security/US_export_policy.jar

    ###  Get LDAPS certificate and import into Truststore  ###
    mkdir -p /opt/cloudera/security/jks
    mkdir -p /opt/cloudera/security/x509
    mkdir -p /opt/cloudera/security/CAcerts
    echo -n | openssl s_client -connect ${DOMAIN_CONTROLLER}:636 | sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' > /opt/cloudera/security/x509/ldaps_cert.pem
    keytool -import -alias ldaps -keystore /opt/cloudera/security/jks/truststore.jks -file /opt/cloudera/security/x509/ldaps_cert.pem -storepass ${TRUSTSTORE_PASS} -noprompt
    ln -s /opt/cloudera/security/jks/truststore.jks /usr/java/jdk1.8.0_60/jre/lib/security/jssecacerts

    ###  Create CSR's for each node  ###
    keytool -genkeypair -keystore /opt/cloudera/security/jks/$(hostname -f).keystore -alias $(hostname -f) -dname "CN=$(hostname -f),OU=Director,O=Cloudera,L=Palo Alto,ST=California,C=US" -ext san=dns:$(hostname -f),dns:*.example.com -keyalg RSA -keysize 2048 -storepass ${KEYSTORE_PASS} -keypass ${KEYSTORE_PASS}
    keytool -certreq -keystore /opt/cloudera/security/jks/$(hostname -f).keystore -alias $(hostname -f) -file /opt/cloudera/security/x509/$(hostname -f).csr -ext san=dns:$(hostname -f),dns:*.example.com -storepass ${KEYSTORE_PASS} -keypass ${KEYSTORE_PASS}
}



#
# Main workflow
#

# ensure user is root
if [ "$(id -u)" -ne 0 ]; then
    echo "Please run as root."
    exit 1
fi

# find the OS and release
os=""
release=""

# if it's there, use lsb_release
rpm -q redhat-lsb
if [ $? -eq 0 ]; then
    os=$(lsb_release -si)
    release=$(lsb_release -sr)

# if lsb_release isn't installed, use /etc/redhat-release
else
    grep  "CentOS.* 6\.7" /etc/redhat-release
    if [ $? -eq 0 ]; then
        os="CentOS"
        release="6.7"
    fi

    grep "CentOS.* 7\.2" /etc/redhat-release
    if [ $? -eq 0 ]; then
        os="CentOS"
        release="7.2"
    fi

    grep "Red Hat Enterprise Linux Server release 6.7" /etc/redhat-release
    if [ $? -eq 0 ]; then
        os="RedHatEnterpriseServer"
        release="6.7"
    fi

    grep "Red Hat Enterprise Linux Server release 7.2" /etc/redhat-release
    if [ $? -eq 0 ]; then
        os="RedHatEnterpriseServer"
        release="7.2"
    fi
fi

# debug
echo $os
echo $release


# shellcheck disable=SC2034
not_supported_msg="OS $os $release is not supported."

# select the OS and run the appropriate setup script
if [ "$os" = "CentOS" ]; then
    if [ "$release" = "6.7" ]; then
        centos_67
    elif [ "$release" = "7.2" ]; then
        centos_72
    else
        echo not_supported_msg
        exit 1
    fi

elif [ "$os" = "RedHatEnterpriseServer" ]; then
    if [ "$release" = "6.7" ]; then
        rhel_67
    elif [ "$release" = "7.2" ]; then
        rhel_72
    else
        echo not_supported_msg
        exit 1
    fi
else
    echo not_supported_msg
    exit 1
fi
"""
} # end bootstrap-script


#
# Instant Templates
#

instances {

    mgmt {
        image: ${?common-instanceTemplate.base.image}
        type: ${?common-instanceTemplate.base.type}
        computeResourceGroup: ${?common-instanceTemplate.management.computeResourceGroup}
        networkSecurityGroupResourceGroup: ${?common-instanceTemplate.base.networkSecurityGroupResourceGroup}
        networkSecurityGroup: ${?common-instanceTemplate.base.networkSecurityGroup}
        virtualNetworkResourceGroup: ${?common-instanceTemplate.base.virtualNetworkResourceGroup}
        virtualNetwork: ${?common-instanceTemplate.base.virtualNetwork}
        subnetName: ${?common-instanceTemplate.base.subnetName}
        instanceNamePrefix: ${?common-instanceTemplate.management.instanceNamePrefix}
        hostFqdnSuffix: ${?common-instanceTemplate.base.hostFqdnSuffix}
        availabilitySet: ${?common-instanceTemplate.management.availabilitySet}
        publicIP: ${?common-instanceTemplate.management.publicIP}
        storageAccountType: ${?common-instanceTemplate.management.storageAccountType}
        dataDiskCount: ${?common-instanceTemplate.management.dataDiskCount}
        dataDiskSize: ${?common-instanceTemplate.management.dataDiskSize}
        tags: ${?common-instanceTemplate.base.tags}
        bootstrapScript: ${?bootstrap-script.os-generic}
    }

    master-1 {
        image: ${?common-instanceTemplate.base.image}
        type: ${?common-instanceTemplate.base.type}
        computeResourceGroup: ${?common-instanceTemplate.master-base.computeResourceGroup}
        networkSecurityGroupResourceGroup: ${?common-instanceTemplate.base.networkSecurityGroupResourceGroup}
        networkSecurityGroup: ${?common-instanceTemplate.base.networkSecurityGroup}
        virtualNetworkResourceGroup: ${?common-instanceTemplate.base.virtualNetworkResourceGroup}
        virtualNetwork: ${?common-instanceTemplate.base.virtualNetwork}
        subnetName: ${?common-instanceTemplate.base.subnetName}
        instanceNamePrefix: ${?common-instanceTemplate.master-base.instanceNamePrefix}
        hostFqdnSuffix: ${?common-instanceTemplate.base.hostFqdnSuffix}
        availabilitySet: ${?common-instanceTemplate.master-base.availabilitySet}
        publicIP: ${?common-instanceTemplate.master-base.publicIP}
        storageAccountType: ${?common-instanceTemplate.master-base.storageAccountType}
        dataDiskCount: ${?common-instanceTemplate.master-1.dataDiskCount}
        dataDiskSize: ${?common-instanceTemplate.master-base.dataDiskSize}
        tags: ${?common-instanceTemplate.base.tags}
        bootstrapScript: ${?bootstrap-script.os-generic}
    }

    master-2 {
        image: ${?common-instanceTemplate.base.image}
        type: ${?common-instanceTemplate.base.type}
        computeResourceGroup: ${?common-instanceTemplate.master-base.computeResourceGroup}
        networkSecurityGroupResourceGroup: ${?common-instanceTemplate.base.networkSecurityGroupResourceGroup}
        networkSecurityGroup: ${?common-instanceTemplate.base.networkSecurityGroup}
        virtualNetworkResourceGroup: ${?common-instanceTemplate.base.virtualNetworkResourceGroup}
        virtualNetwork: ${?common-instanceTemplate.base.virtualNetwork}
        subnetName: ${?common-instanceTemplate.base.subnetName}
        instanceNamePrefix: ${?common-instanceTemplate.master-base.instanceNamePrefix}
        hostFqdnSuffix: ${?common-instanceTemplate.base.hostFqdnSuffix}
        availabilitySet: ${?common-instanceTemplate.master-base.availabilitySet}
        publicIP: ${?common-instanceTemplate.master-base.publicIP}
        storageAccountType: ${?common-instanceTemplate.master-base.storageAccountType}
        dataDiskCount: ${?common-instanceTemplate.master-2.dataDiskCount}
        dataDiskSize: ${?common-instanceTemplate.master-base.dataDiskSize}
        tags: ${?common-instanceTemplate.base.tags}
        bootstrapScript: ${?bootstrap-script.os-generic}
    }

    worker {
        image: ${?common-instanceTemplate.base.image}
        type: ${?common-instanceTemplate.base.type}
        computeResourceGroup: ${?common-instanceTemplate.worker.computeResourceGroup}
        networkSecurityGroupResourceGroup: ${?common-instanceTemplate.base.networkSecurityGroupResourceGroup}
        networkSecurityGroup: ${?common-instanceTemplate.base.networkSecurityGroup}
        virtualNetworkResourceGroup: ${?common-instanceTemplate.base.virtualNetworkResourceGroup}
        virtualNetwork: ${?common-instanceTemplate.base.virtualNetwork}
        subnetName: ${?common-instanceTemplate.base.subnetName}
        instanceNamePrefix: ${?common-instanceTemplate.worker.instanceNamePrefix}
        hostFqdnSuffix: ${?common-instanceTemplate.base.hostFqdnSuffix}
        availabilitySet: ${?common-instanceTemplate.worker.availabilitySet}
        publicIP: ${?common-instanceTemplate.worker.publicIP}
        storageAccountType: ${?common-instanceTemplate.worker.storageAccountType}
        dataDiskCount: ${?common-instanceTemplate.worker.dataDiskCount}
        dataDiskSize: ${?common-instanceTemplate.worker.dataDiskSize}
        tags: ${?common-instanceTemplate.base.tags}
        bootstrapScript: ${?bootstrap-script.os-generic}
    }

    edge {
        image: ${?common-instanceTemplate.base.image}
        type: ${?common-instanceTemplate.base.type}
        computeResourceGroup: ${?common-instanceTemplate.edge.computeResourceGroup}
        networkSecurityGroupResourceGroup: ${?common-instanceTemplate.base.networkSecurityGroupResourceGroup}
        networkSecurityGroup: ${?common-instanceTemplate.base.networkSecurityGroup}
        virtualNetworkResourceGroup: ${?common-instanceTemplate.base.virtualNetworkResourceGroup}
        virtualNetwork: ${?common-instanceTemplate.base.virtualNetwork}
        subnetName: ${?common-instanceTemplate.base.subnetName}
        instanceNamePrefix: ${?common-instanceTemplate.edge.instanceNamePrefix}
        hostFqdnSuffix: ${?common-instanceTemplate.base.hostFqdnSuffix}
        availabilitySet: ${?common-instanceTemplate.edge.availabilitySet}
        publicIP: ${?common-instanceTemplate.edge.publicIP}
        storageAccountType: ${?common-instanceTemplate.edge.storageAccountType}
        dataDiskCount: ${?common-instanceTemplate.edge.dataDiskCount}
        dataDiskSize: ${?common-instanceTemplate.edge.dataDiskSize}
        tags: ${?common-instanceTemplate.base.tags}
        bootstrapScript: ${?bootstrap-script.os-generic}
    }

} # End instance templates


#
# Required external database server configuration.
#
# Cloudera Director can create databases on existing database servers.
# NOTE: Cloudera does not support Azure SQL DB service.
#

databaseServers {

    mysqldirector {
        type: ${DB_TYPE}
        host: ${DB_IP} # Cloudera recommends using the static IP address of database server
        port: ${DB_PORT}
        user: ${DB_USER}
        password: ${DB_PASS}
    }

} # End external database configs


#
# Configuration for Cloudera Manager. Cloudera Director can use an existing Cloudera Manager
# or bootstrap everything from scratch for a new cluster
#

cloudera-manager {

    instance: ${instances.mgmt} {
        tags {
            application: "Cloudera Manager 5"
        }
    }

    #
    # Licensing configuration
    #
    # There are three mutually exclusive options for setting up Cloudera Manager's license.
    # 1. License text may be embedded in this file using the "license" field. Triple quotes (""")
    #    are recommended for including multi-line text strings.
    # 2. The "licensePath" can be used to specify the path to a file containing the license.
    # 3. The "enableEnterpriseTrial" flag indicates whether the 60-Day Cloudera Enterprise Trial
    #    should be activated when no license is present. This must not be set to true if a
    #    license is included using either "license" or "licensePath".

    #
    # Embed a license for Cloudera Manager
    #

    # license: """
    #   -----BEGIN PGP SIGNED MESSAGE-----
    #   Hash: SHA1
    #
    # {
    #   "version"        : 1,
    #   "name"           : "License Owner",
    #   "uuid"           : "license id",
    #   "expirationDate" : 0,
    #   "features"       : [ "FEATURE1", "FEATURE2" ]
    # }
    # -----BEGIN PGP SIGNATURE-----
    # Version: GnuPG v1.4.11 (GNU/Linux)
    #
    # PGP SIGNATURE
    # -----END PGP SIGNATURE-----
    # """


    #
    # Include a license for Cloudera Manager from an external file
    #

    #licensePath: "/root/license.txt.asc"
    licensePath: ${CLOUDERA_LICENSE_PATH}               # REPLACE_ME if the license file is not in this location or if you don't have a license see comments above about license options.

    #
    # Specify the billingId.
    #
    # Cloudera Director will use the billing ID to report usage information to a metering service
    # for usage based billing.
    #
    # Usage reporting starts as soon as you assign a billing ID and a license to a Cloudera Manager.
    # If you remove a billing ID, Director will stop reporting to the metering service.
    #
    # When usage reporting stops, you will not have access to Cloudera Support with this deployment.
    # If you want a billing ID, please contact Cloudera.
    #

    # billingId: "billingId_REPLACE_ME"

    #
    # Activate 60-Day Cloudera Enterprise Trial
    #

    enableEnterpriseTrial: false

    # Deployment option for Java when deploying via Cloudera Director.
    # AUTO - default, will install Java 6 and 7
    # NONE - requires custom bootstrap script to install desired Java version

    javaInstallationStrategy: NONE

    #
    # Install the unlimited strength JCE policy files for higher levels of encryption.
    # Prior to setting this to true, confirm that you understand the legal ramifications
    # of using unlimited JCE policy files in your country.
    #

    unlimitedJce: false

    # An administrative Kerberos account capable of creating principals on the KDC that
    # Cloudera Manager will be using. This will typically be in the format:
    #    Principal@YOUR.KDC.REALM

    krbAdminUsername: ${KERBEROS_ADMIN_USER}

    # The password for the administrative Kerberos account.

    krbAdminPassword: ${KERBEROS_ADMIN_PASS}


    #
    # Optional database configuration
    #
    # There are three mutually exclusive options for database usage in Cloudera Director.
    # 1. This option is NOT supported for production use.
    #    With no configuration, an embedded PostgreSQL database will be used.
    # 2. Alternatively, existing external databases can be used.
    # 3. Finally, databases can be created on the fly on existing external database servers.

    #
    # Optional configuration for existing external databases
    #
    # databases {
    #     CLOUDERA_MANAGER {
    #         type: postgresql
    #
    #         host: db.example.com
    #         port: 123
    #
    #         user: admin
    #         password: 1231ed
    #
    #         name: scm
    #     }
    #
    #     ACTIVITYMONITOR { ... }
    #
    #     REPORTSMANAGER { ... }
    #
    #     NAVIGATOR { ... }
    #
    #     # Added in Cloudera Manager 5.2+
    #     NAVIGATORMETASERVER { ... }
    # }

    #
    # Optional configuration for creating external databases on the fly
    #
    # When a database is created on the fly, Director generates a random database name using the specified database
    # name prefix, a random username based on the specified username prefix, and a random password. The password is
    # stored by Director and made available to the service that uses the database. If multiple services reference the
    # same external database server, Director will create a database for each.
    #
    # MySQL limits usernames to sixteen characters. Therefore, limit usernamePrefix values for databases on MySQL to
    # seven characters; the remaining nine characters are used by the randomized suffix generated by Director.
    #

    databaseTemplates {
        CLOUDERA_MANAGER {
            name: cmtemplate
            databaseServerName: mysqldirector # Must correspond to an external database server named above
            databaseNamePrefix: scm
            usernamePrefix: cmadmin
        }

        ACTIVITYMONITOR {
            name: amontemplate
            databaseServerName: mysqldirector # Must correspond to an external database server named above
            databaseNamePrefix: amon
            usernamePrefix: amadmin
        }

        REPORTSMANAGER {
            name: rmantemplate
            databaseServerName: mysqldirector # Must correspond to an external database server named above
            databaseNamePrefix: rman
            usernamePrefix: rmadmin
        }

        NAVIGATOR {
            name: navtemplate
            databaseServerName: mysqldirector # Must correspond to an external database server named above
            databaseNamePrefix: nav
            usernamePrefix: nadmin
        }

        # Added in Cloudera Manager 5.2+
        NAVIGATORMETASERVER {
            name: navmetatemplate
            databaseServerName: mysqldirector # Must correspond to an external database server named above
            databaseNamePrefix: navmeta
            usernamePrefix: nmadmin
        }
    }

    #
    # Configuration to override Cloudera Manager package repositories
    #

    repository: ${CM_REPO}
    repositoryKeyUrl: ${CM_REPO_KEY}

    # OR use an existing Cloudera Manager installation

    # hostname: "192.168.33.10"
    # username: <if not default 'admin'>
    # password: <if not default 'admin'>

    #
    # Optional configuration for Cloudera Manager and its management services
    #
    # Configuration properties for CLOUDERA_MANAGER are documented at
    # http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/cm_props_cmserver.html
    #
    # Configuration properties for the Cloudera Management services are documented at
    # http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/cm_props_mgmtservice.html
    #
    # Configuration properties for Hosts are documented at
    # http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/cm_props_host.html
    #
    configs {
        # CLOUDERA_MANAGER corresponds to the Cloudera Manager Server configuration options
        CLOUDERA_MANAGER {
            # enable_api_debug: false
            custom_banner_html: "Managed by Cloudera Director"
            # Kerberos Settings:

            # The type of KDC Cloudera Manager will be using. Valid values are "MIT KDC"
            # and "Active Directory"

            KDC_TYPE: ${KDC_TYPE}

            # The KDC host name or IP address.

            KDC_HOST: ${KDC_HOST}

            # The security realm that your KDC uses. This will be of the format of a fully
            # qualified domain name:
            #    YOUR.KDC.REALM

            SECURITY_REALM: ${KDC_REALM}

            # The Active Directory KDC domain. Only applicable to Active Directory KDCs. This
            # will be in the format of an X.500 Directory Specification:
            #    DC=domain,DC=example,DC=com

            AD_KDC_DOMAIN: ${KDC_AD_DOMAIN}

            # Allow Cloudera Manager to deploy Kerberos configurations to hosts. This should
            # be set to true unless you have an alternate mechanism to generate or retrieve the
            # Kerberos configuration on your Cloudera Manager node instances.

            KRB_MANAGE_KRB5_CONF: true

            # The encryption types your KDC supports. Some of those listed below will require the
            # unlimited strength JCE policy files.
            #KRB_ENC_TYPES: "aes256-cts aes128-cts des3-hmac-sha1 arcfour-hmac des-hmac-sha1 des-cbc-md5 des-cbc-crc"

            KRB_ENC_TYPES: ${KRB_ENC_TYPES}

            # There are many more optional Kerberos configuration options available to Cloudera Manager.
            # Please refer to the Kerberos section on
            # http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/cm_props_cmserver.html
            # for more details.

            AD_ACCOUNT_PREFIX: "cdh-"
            AD_DELETE_ON_REGENERATE: "true"
            AD_SET_ENCRYPTION_TYPES: "true"
            AD_USE_SIMPLE_AUTH: "false"
            KDC_ACCOUNT_CREATION_HOST_OVERRIDE: ${KDC_HOST}


            ###  Configuring External Authentiction for Cloudera Manager   ###
            AUTH_BACKEND_ORDER: "LDAP_ONLY"
            LDAP_URL: ${LDAP_URL}
            NT_DOMAIN: ${KDC_REALM}
            LDAP_BIND_DN: ${LDAP_ADMIN_USER}
            LDAP_BIND_PW: ${LDAP_ADMIN_PASS}
            LDAP_ADMIN_GROUPS: ${CM_ADMIN_GROUP}
            LDAP_USER_GROUPS: ${CM_USER_GROUPS}
            LDAP_NAVIGATOR_ADMIN_GROUPS: ${NAV_ADMIN_GROUPS}
            LDAP_GROUP_SEARCH_FILTER: "(member={0})"
            LDAP_USER_SEARCH_FILTER: "(uid={0})"
        }

        # CLOUDERA_MANAGEMENT_SERVICE corresponds to the Service-Wide configuration options
        CLOUDERA_MANAGEMENT_SERVICE {
            enable_alerts : false
            enable_config_alerts : false
        }

        SERVICEMONITOR {
            mgmt_log_dir: /data0/log/cloudera-scm-firehose
            firehose_storage_dir: /data0/lib/cloudera-service-monitor
        }

        ACTIVITYMONITOR {
            mgmt_log_dir: /data0/log/cloudera-scm-firehose
        }

        HOSTMONITOR {
            mgmt_log_dir: /data0/log/cloudera-scm-firehose
            firehose_storage_dir: /data0/lib/cloudera-host-monitor
        }

        REPORTSMANAGER {
            headlamp_scratch_dir: /data0/lib/cloudera-scm-headlamp
            mgmt_log_dir: /data0/log/cloudera-scm-headlamp
        }

        EVENTSERVER {
            mgmt_log_dir: /data0/log/cloudera-scm-eventserver
            eventserver_index_dir: /data0/lib/cloudera-scm-eventserver
        }

        ALERTPUBLISHER {
            mgmt_log_dir: /data0/log/cloudera-scm-alertpublisher
        }

        NAVIGATOR {
            mgmt_log_dir: /data0/log/cloudera-scm-navigator
        }

        NAVIGATORMETASERVER {
            audit_event_log_dir: /data0/log/cloudera-scm-navigator/audit
            data_dir: /data0/lib/cloudera-scm-navigator
            mgmt_log_dir: /data0/log/cloudera-scm-navigator
            auth_backend_order: "EXTERNAL_THEN_CM"
            external_auth_type: "ACTIVE_DIRECTORY"
            nav_ldap_url: ${NAV_LDAP_URL}
            nav_nt_domain: ${KDC_REALM}
            nav_ldap_bind_dn: ${LDAP_ADMIN_USER}
            nav_ldap_bind_pw: ${LDAP_ADMIN_PASS}
            nav_ldap_group_search_base: ${NAV_LDAP_GROUP_SEARCH_BASE}
            nav_ldap_group_search_filter: "(memberOf={0})"
            nav_ldap_groups_search_filter: "(&(objectClass=group)(cn=*{0}*))"
            nav_ldap_user_search_base: ${NAV_LDAP_USER_SEARCH_BASE}
            nav_ldap_user_search_filter: "(sAMAccountName={0})"
        }

        # Configuration properties for all hosts
        HOSTS {
        }
    }
} # End CM configuration

#
# Highly Available Cluster description
#

cluster {

    # List the products and their versions that need to be installed.
    # These products must have a corresponding parcel in the parcelRepositories
    # configured above. The specified version will be used to find a suitable
    # parcel. Specifying a version that points to more than one parcel among
    # those available will result in a configuration error. Specify more granular
    # versions to avoid conflicts.

    products {
        CDH: ${CDH_VERSION}
    }

    #
    # Optional override of CDH parcel repositories
    #

    parcelRepositories: ["CDH_REPO"]

    services: [CLUSTER_SERVICES]                                        ### HDFS, YARN, ZOOKEEPER, HIVE, HUE, OOZIE, SPARK_ON_YARN, IMPALA, FLUME, HBASE, SOLR, SQOOP

    #
    # Optional custom service configurations
    # Configuration keys containing periods must be enclosed in double quotes.
    #

    configs {
        # HDFS fencing should be set to true for HA configurations
        HDFS {
            dfs_ha_fencing_methods: "shell(true)"
            dfs_replication: "3"
            dfs_block_local_path_access_user: "impala,mapred,spark"
            dfs_namenode_acls_enabled: "true"
            dfs_permissions_supergroup: ${HDFS_SUPERGROUP}
            #dfs_umaskmode: "027"                                       ### Do not set this, will only start HDFS and not allow other services to succeed during the First Run of the cluster.  Should be set after the cluster has completed deployment via Director.
            hadoop_authorized_admin_groups: ${HDFS_ADMIN_GROUPS}
            hadoop_authorized_admin_users: ""
            hadoop_authorized_groups: ${HDFS_AUTHORIZED_GROUPS}
            hadoop_authorized_users: ""
            hadoop_secure_web_ui: "true"
            hdfs_sentry_sync_enable: "true"
            redaction_policy_enabled: "true"
            redaction_policy: "{\n  \"version\": 1,\n  \"rules\": [\n    {\n      \"description\": \"Credit Card numbers (with separator)\",\n      \"search\": \"\\\\d{4}[^\\\\w]\\\\d{4}[^\\\\w]\\\\d{4}[^\\\\w]\\\\d{4}\",\n      \"caseSensitive\": true,\n      \"replace\": \"XXXX-XXXX-XXXX-XXXX\"\n    },\n    {\n      \"description\": \"Email addresses\",\n      \"search\": \"\\\\b([A-Za-z0-9]|[A-Za-z0-9][A-Za-z0-9\\\\-\\\\._]*[A-Za-z0-9])@(([A-Za-z0-9]|[A-Za-z][A-Za-z0-9\\\\-]*[A-Za-z0-9])\\\\.)+([A-Za-z0-9]|[A-Za-z0-9][A-Za-z0-9\\\\-]*[A-Za-z0-9])\\\\b\",\n      \"caseSensitive\": true,\n      \"replace\": \"email@redacted.host\"\n    },\n    {\n      \"description\": \"Social Security numbers (with separator)\",\n      \"search\": \"\\\\d{3}[^\\\\w]\\\\d{2}[^\\\\w]\\\\d{4}\",\n      \"caseSensitive\": true,\n      \"replace\": \"XXX-XX-XXXX\"\n    }\n  ]\n}"
        }

        # Configuring OOZIE high availability (optional) requires a load balancer to be specified
        # Director does not create or manage the load balancer.
        #
        # The load balancer must be configured with the IPs of the oozie servers
        # after the cluster completes bootstrapping.
        # OOZIE {
        #     oozie_load_balancer: "oozie_load_balancer_REPLACE_ME:11000"
        # }

        YARN {
            hadoop_secure_web_ui: "true"
            yarn_admin_acl: ${YARN_ADMIN_GROUPS}
        }

        HIVE {
            audit_event_log_dir: /data0/log/hive/audit
            lineage_event_log_dir: /data0/log/hive/lineage
        }

        HUE {
            auth_backend: "desktop.auth.backend.LdapBackend"
            ldap_url: ${HUE_LDAP_URL}
            use_start_tls: ${HUE_LDAPS_FLAG}
            bind_dn: ${HUE_LDAP_ADMIN_USER}
            bind_password: ${LDAP_ADMIN_PASS}
            base_dn: ${HUE_LDAP_SEARCH_BASE}
            create_users_on_login: "false"
            nt_domain: ${KDC_REALM}
            search_bind_authentication: "true"
        }

        SENTRY {
            sentry_service_admin_group: ${SENTRY_ADMIN_GROUPS}
        }

        #HBASE {
        #    audit_event_log_dir: /data0/log/hbase/audit
        #}
    }

    #
    # High availibility configuration requires external databases to be defined for the
    # Hive Metastore, Hue, and Oozie services. These databases may be configured either
    # as existing external databases using the "databases" block below, or as "databaseTemplate"
    # to be created on a databaseServer.
    #

    #
    # Optional configuration for existing external database for Hive Metastore, Hue,
    # and Oozie databases
    #

    # databases {
    #     HIVE {
    #         type: postgresql
    #         host: db.example.com
    #         port: 123
    #         user: hive
    #         password: pass
    #         name: hive_db
    #     }
    #     HUE {
    #         type: postgresql
    #         host: db.example.com
    #         port: 123
    #         user: hue
    #         password: pass
    #         name: hue_db
    #     }
    #     OOZIE {
    #         type: postgresql
    #         host: db.example.com
    #         port: 123
    #         user: oozie
    #         password: pass
    #         name: oozie_db
    #     }
    # }

    #
    # Optional configuration for creating external database on the fly for Hive Metastore
    # Hue, and Oozie databases
    #

    databaseTemplates: {
        HIVE {
            name: hivetemplate
            databaseServerName: mysqldirector # Must correspond to an external database server named above
            databaseNamePrefix: hivemetastore
            usernamePrefix: hive
        }

        HUE {
            name: huetemplate
            databaseServerName: mysqldirector # Must correspond to an external database server named above
            databaseNamePrefix: huedb
            usernamePrefix: hue
        }

        OOZIE {
            name: oozietemplate
            databaseServerName: mysqldirector # Must correspond to an external database server named above
            databaseNamePrefix: ooziedb
            usernamePrefix: oozie
        }

        SENTRY {
            name: sentrytemplate
            databaseServerName: mysqldirector # Must correspond to an external database server named above
            databaseNamePrefix: sentrydb
            usernamePrefix: sentry
        }

    }

    #
    # This reference configuration follows the Cloudera Azure Reference Architecture.
    #

    masters-1 {
        count: ${MASTER_HA_NODE_COUNT}

        instance: ${instances.master-1} {
            tags {
                group: masters-1
            }
        }

        roles {
            ZOOKEEPER: [SERVER]
            HDFS: [NAMENODE, FAILOVERCONTROLLER, JOURNALNODE]
            YARN: [RESOURCEMANAGER]
            #HBASE: [MASTER]
        }

        # NameNode nameservice, autofailover, and quorum journal name must be configured for high availability
        configs {
            HDFS {
                NAMENODE {
                    dfs_federation_namenode_nameservice: ${HDFS_NAMESERVICE}
                    autofailover_enabled: true
                    dfs_namenode_quorum_journal_name: ${HDFS_NAMESERVICE}

                    namenode_log_dir: /data0/log/hadoop-hdfs
                    dfs_name_dir_list: /data3/dfs/nn
                }
                FAILOVERCONTROLLER
                {
                    failover_controller_log_dir: /data0/log/hadoop-hdfs
                }
                JOURNALNODE
                {
                  journalnode_log_dir: /data0/log/hadoop-hdfs
                  dfs_journalnode_edits_dir: /data1/hdfs
                }
            }
            ZOOKEEPER {
                SERVER {
                    zk_server_log_dir: /data0/log/zookeeper
                    dataDir: /data2/zookeeper
                    dataLogDir: /data2/zookeeper
                    maxClientCnxns: 1024
                }
            }
            YARN {
              RESOURCEMANAGER {
                resource_manager_log_dir: /data0/log/hadoop-yarn
              }
            }
            #HBASE {
            #  MASTER {
            #    hbase_master_log_dir: /data0/log/hbase
            #  }
            #}
        }
    }

    masters-2 {
        count: ${MASTER_NODE_COUNT}

        instance: ${instances.master-2} {
            tags {
                group: masters-2
            }
        }

        roles {
            ZOOKEEPER: [SERVER]
            HDFS: [JOURNALNODE, HTTPFS]
            HIVE: [HIVESERVER2, HIVEMETASTORE, WEBHCAT]
            YARN: [JOBHISTORY]
            HUE: [HUE_SERVER]
            OOZIE: [OOZIE_SERVER]
            IMPALA: [CATALOGSERVER, STATESTORE]
            SPARK_ON_YARN: [SPARK_YARN_HISTORY_SERVER]
            #HBASE: [HBASETHRIFTSERVER] # Alternately [HBASERESTSERVER], for HUE Integration
            SENTRY: [SENTRY_SERVER]
        }

        # Oozie plugins must be configured for high availability
        configs {
            HDFS {
                JOURNALNODE
                {
                    journalnode_log_dir: /data0/log/hadoop-hdfs
                    dfs_journalnode_edits_dir: /data1/hdfs
                }
                HTTPFS
                {
                    httpfs_log_dir: /data0/log/hadoop-httpfs
                }
            }
            OOZIE {
                OOZIE_SERVER {
                    oozie_plugins_list: "org.apache.oozie.service.ZKLocksService,org.apache.oozie.service.ZKXLogStreamingService,org.apache.oozie.service.ZKJobsConcurrencyService,org.apache.oozie.service.ZKUUIDService"
                    oozie_log_dir: /data0/log/oozie
                }
            }
            ZOOKEEPER {
                SERVER {
                    zk_server_log_dir: /data0/log/zookeeper
                    dataDir: /data2/zookeeper
                    dataLogDir: /data2/zookeeper
                    maxClientCnxns: 1024
                }
            }
            HIVE {
                HIVEMETASTORE {
                    hive_log_dir: /data0/log/hive
                }
                HIVESERVER2 {
                    hive_log_dir: /data0/log/hive
                }
                WEBHCAT {
                    hcatalog_log_dir: /data0/log/hcatalog
                }
            }
            YARN {
                JOBHISTORY {
                    mr2_jobhistory_log_dir: /data0/log/hadoop-mapreduce
                }
            }
            HUE {
                HUE_SERVER {
                    hue_server_log_dir: /data0/log/hue
                }
                KT_RENEWER {
                    kt_renewer_log_dir: /data0/log/hue
                }
            }
            IMPALA {
                CATALOGSERVER {
                    log_dir: /data0/log/catalogd
                }
                STATESTORE {
                    log_dir: /data0/log/statestore
                }
            }
            SPARK_ON_YARN {
                SPARK_YARN_HISTORY_SERVER {
                    log_dir: /data0/log/spark
                }
            }
            #HBASE {
            #    HBASETHRIFTSERVER {
            #        hbase_thriftserver_log_dir: /data0/log/hbase
            #    }
            #    #HBASERESTSERVER {
            #    #    hbase_restserver_log_dir: /data0/log/hbase
            #    #}
            #}
        }
    }

    workers {
        #
        # The desired number of instances to provision. Cloudera Director will attempt to allocate
        # this many instances but will not fail the deployment as long as the minimum number of
        # instances (specified with minCount below) is allocated. If minCount is not specified
        # then minCount is set to count.
        #
        count: ${WORKER_NODE_COUNT}

        #
        # Minimum number of instances required to set up the cluster.
        # Fail and quit if minCount number of instances is not available in this cloud
        # environment. Else, continue setting up the cluster. If minCount is not defined then
        # minCount defaults to count.
        #
        minCount: 4

        instance: ${instances.worker} {
            tags {
                group: worker
            }
        }

        roles {
            HDFS: [DATANODE]
            YARN: [NODEMANAGER]
            #HBASE: [REGIONSERVER]
            IMPALA: [IMPALAD]
        }

        # Optional custom role configurations
        # Configuration keys containing periods must be enclosed in double quotes.
        configs {
            HDFS {
                DATANODE {
                    datanode_log_dir: /data0/log/hadoop-hdfs
                    dfs_data_dir_list: "/data1/dfs/dn,/data2/dfs/dn,/data3/dfs/dn,/data4/dfs/dn,/data5/dfs/dn,/data6/dfs/dn,/data7/dfs/dn,/data8/dfs/dn,/data9/dfs/dn,/data10/dfs/dn"
                    dfs_datanode_failed_volumes_tolerated: 5            # Should be half the number of data drives
                }
            }
            YARN {
                NODEMANAGER {
                    node_manager_log_dir: /data0/log/hadoop-yarn
                    yarn_nodemanager_log_dirs: "/data1/log/hadoop-yarn/container,/data2/log/hadoop-yarn/container,/data3/log/hadoop-yarn/container,/data4/log/hadoop-yarn/container,/data5/log/hadoop-yarn/container,/data6/log/hadoop-yarn/container,/data7/log/hadoop-yarn/container,/data8/log/hadoop-yarn/container,/data9/log/hadoop-yarn/container,/data10/log/hadoop-yarn/container"
                    yarn_nodemanager_local_dirs: "/data1/yarn,/data2/yarn,/data3/yarn,/data4/yarn,/data5/yarn,/data6/yarn,/data7/yarn,/data8/yarn,/data9/yarn,/data10/yarn"
                }
            }
            #HBASE {
            #    REGIONSERVER {
            #        hbase_regionserver_log_dir: /data0/log/hbase
            #    }
            #}
            IMPALA {
                IMPALAD {
                    log_dir: /data0/log/impalad
                    lineage_event_log_dir: /data0/log/impalad/lineage
                    audit_event_log_dir: /data0/log/impalad/audit
                    scratch_dirs: "/data1/impala/impalad,/data2/impala/impalad,/data3/impala/impalad,/data4/impala/impalad,/data5/impala/impalad,/data6/impala/impalad,/data7/impala/impalad,/data8/impala/impalad,/data9/impala/impalad,/data10/impala/impalad"
                }
            }
        }
    }

    postCreateScripts: ["""#!/bin/sh

# This is an embedded post creation script that runs as root and can be used to
# customize the cluster after it has been created.

# If the exit code is not zero Cloudera Director will fail

# Post creation scripts also have access to the following environment variables:

#    DEPLOYMENT_HOST_PORT
#    ENVIRONMENT_NAME
#    DEPLOYMENT_NAME
#    CLUSTER_NAME
#    CM_USERNAME
#    CM_PASSWORD

# Mention repo ip below
wget http://${REPO_IP}:${REPO_PORT}/ssh/privatekey.pem -O /tmp/privatekey.pem
chmod 600 /tmp/privatekey.pem
CM_FQDN=$(grep server_host /etc/cloudera-scm-agent/config.ini | awk -F'=' '{print $2}')
curl -u admin:admin -k http://${CM_FQDN}:7180/api/v13/cm/deployment > /tmp/cm_deployment.txt
HUE_HOST_ID=$(grep -A 5 "\"type\" : \"HUE_SERVER\"" /tmp/cm_deployment.txt | grep hostId | awk -F':' '{print $2}' | sed -e 's/"//g' -e 's/ //g')
HUE_IP=$(grep -A 5 ${HUE_HOST_ID} /tmp/cm_deployment.txt | grep ipAddress | awk -F':' '{print $2}' | sed -e 's/"//g' -e 's/ //g' -e 's/,//g')
CLUSTER_NAME=$(grep -A 1 "\"clusters\"" /tmp/cm_deployment.txt | grep name | awk -F':' '{print $2}' | sed -e 's/"//g' -e 's/^ //g' -e 's/ /%20/g' -e 's/,//g')
SERVICE_NAME=$(grep -B 1 "\"type\" : \"HUE\"" /tmp/cm_deployment.txt | grep name | awk -F':' '{print $2}' | sed -e 's/"//g' -e 's/ //g' -e 's/,//g')

###  Change owner to Hue log director to Hue and restart Hue Kerberos Ticket Renewer  ###
ssh -oStrictHostKeyChecking=no -i /tmp/privatekey.pem -t cloudera-scm@${HUE_IP} 'sudo chown hue:hue /var/log/hue'
curl -X POST -u admin:admin -k http://${CM_FQDN}:7180/api/v13/clusters/${CLUSTER_NAME}/services/${SERVICE_NAME}/commands/restart

###  Restart Cloudera Manager Server to enable External Authentication via AD/LDAP  ###
ssh -oStrictHostKeyChecking=no -i /tmp/privatekey.pem -t cloudera-scm@${CM_FQDN} 'sudo systemctl restart cloudera-scm-server'

###  Change ownership and mode to Hive Warehouse Directory for Sentry Synchronization  ###
kinit ${KERBEROS_ADMIN_USER} <<EOF
${KERBEROS_ADMIN_PASS}
EOF                                          ## This needs to be changed to a keytab!!
hdfs dfs -chmod -R 771 /user/hive/warehouse
hdfs dfs -chown -R hive:hive /user/hive/warehouse

exit 0
    """,
    """#!/usr/bin/python

# Additionally, multiple post-creation scripts can be supplied.  They will run
# in the order they are listed here.  Interpeters other than bash can be used
# as well.

print 'Hello again!'
    """]

    # For more complex scripts, post creation scripts can be supplied via path,
    # where they will be read from the local filesystem.  They will run after
    # any scripts supplied in the previous postCreateScripts section.
    # postCreateScriptsPaths: ["/tmp/test-script.sh",
    #                         "/tmp/test-script.py"]

    preTerminateScripts: ["""#!/bin/sh

# This is an embedded pre-termination script that runs as root and can be used to
# customize the cluster after it has been created.

# If the exit code is not zero Cloudera Director will fail

# Pre terminate scripts also have access to the following environment variables:

#    DEPLOYMENT_HOST_PORT
#    ENVIRONMENT_NAME
#    DEPLOYMENT_NAME
#    CLUSTER_NAME
#    CM_USERNAME
#    CM_PASSWORD

echo 'Goodbye World!'
exit 0
    """,
    """#!/usr/bin/python

# Additionally, multiple pre terminate scripts can be supplied.  They will run
# in the order they are listed here.  Interpeters other than bash can be used
# as well.

print 'Goodbye again!'
        """]

    # For more complex scripts, pre terminate scripts can be supplied via path,
    # where they will be read from the local filesystem.  They will run after
    # any scripts supplied in the previous preTerminateScripts section.
    # preTerminateScriptsPaths: ["/tmp/test-script.sh",
    #                            "/tmp/test-script.py"]
}
